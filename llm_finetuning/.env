MODEL_NAME=gpt2
DATASET_NAME=wikitext
DATASET_CONFIG=wikitext-2-raw-v1
OUTPUT_DIR=".\\outputs"
LOGGING_DIR=".\\logs"
BATCH_SIZE=8
EPOCHS=3
SAVE_STEPS=10000
LORA_R=8
LORA_ALPHA=32
LORA_DROPOUT=0.1
LORA_BIAS=none
